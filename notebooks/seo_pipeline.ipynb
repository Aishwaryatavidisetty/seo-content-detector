{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573fa67c",
   "metadata": {},
   "source": [
    "# SEO Content Detector Pipeline\n",
    "_Notebook: `seo_pipeline.ipynb`_\n",
    "\n",
    "This notebook implements the full assignment pipeline:\n",
    "A) Setup & Data Load → B) HTML → Clean Text → C) Feature Engineering → D) Duplicate Detection → E) Quality Labels & Model → F) Real-time `analyze_url(url)` Demo → G) Save Outputs\n",
    "\n",
    "**Repo layout** (expected):\n",
    "```\n",
    "seo-content-detector/\n",
    "├─ data/\n",
    "│  ├─ data.csv\n",
    "│  ├─ extracted_content.csv\n",
    "│  ├─ features.csv\n",
    "│  └─ duplicates.csv\n",
    "├─ notebooks/\n",
    "│  └─ seo_pipeline.ipynb\n",
    "├─ models/\n",
    "│  └─ quality_model.pkl (created later)\n",
    "└─ requirements.txt (recommended)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003fecb2",
   "metadata": {},
   "source": [
    "## A. Setup & Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09c89da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tldextract\n",
      "  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-3.0.1-py2.py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from tldextract) (2.10)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from tldextract) (3.0.12)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from tldextract) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract) (1.26.4)\n",
      "Installing collected packages: requests-file, tldextract\n",
      "Successfully installed requests-file-3.0.1 tldextract-5.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9dfce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.10-py3-none-any.whl (239 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from textstat) (3.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from textstat) (52.0.0.post20210125)\n",
      "Collecting pyphen\n",
      "  Downloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: regex in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from nltk->textstat) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from nltk->textstat) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from nltk->textstat) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\aishwarya\\anaconda3\\lib\\site-packages (from nltk->textstat) (7.1.2)\n",
      "Installing collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.16.0 textstat-0.7.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4108f32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 81 | Columns: ['url', 'html_content']\n",
      "Has html_content: True\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, re, json, time, math, itertools, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import tldextract\n",
    "import textstat\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# NLTK downloads (quiet)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path(\"C:/Users/AISHWARYA/Downloads/seo-content-detector\")\n",
    "DATA = ROOT / \"data\"\n",
    "MODELS = ROOT / \"models\"\n",
    "\n",
    "DATA.mkdir(parents=True, exist_ok=True)\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "data_path = DATA / \"data.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "has_html = 'html_content' in df.columns\n",
    "print(\"Rows:\", len(df), \"| Columns:\", list(df.columns))\n",
    "print(\"Has html_content:\", has_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc78f5",
   "metadata": {},
   "source": [
    "## B. HTML → Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f608fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted content to: C:\\Users\\AISHWARYA\\Downloads\\seo-content-detector\\data\\extracted_content.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>Cyber Security Blog</td>\n",
       "      <td>Cybersecurity Blog</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-91.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
       "      <td>Blog Privacy &amp; Compliance Top 10 Cybersecurity...</td>\n",
       "      <td>1747</td>\n",
       "      <td>94</td>\n",
       "      <td>40.871699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
       "      <td>Home Insights Blog Posts 11 Cyber Defense Tips...</td>\n",
       "      <td>1058</td>\n",
       "      <td>73</td>\n",
       "      <td>53.262918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>Cybersecurity Best Practices | Cybersecurity a...</td>\n",
       "      <td>Cybersecurity Best Practices CISA provides inf...</td>\n",
       "      <td>779</td>\n",
       "      <td>27</td>\n",
       "      <td>1.035698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                Cyber Security Blog   \n",
       "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
       "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
       "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
       "4                                                      \n",
       "\n",
       "                                           body_text  word_count  \\\n",
       "0                                 Cybersecurity Blog           2   \n",
       "1  Blog Privacy & Compliance Top 10 Cybersecurity...        1747   \n",
       "2  Home Insights Blog Posts 11 Cyber Defense Tips...        1058   \n",
       "3  Cybersecurity Best Practices CISA provides inf...         779   \n",
       "4                                                              0   \n",
       "\n",
       "   sentence_count  flesch_reading_ease  \n",
       "0               1           -91.295000  \n",
       "1              94            40.871699  \n",
       "2              73            53.262918  \n",
       "3              27             1.035698  \n",
       "4               0             0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ensure the primary dataset shape is correct\n",
    "assert has_html, \"Primary dataset expected: data.csv must include an 'html_content' column.\"\n",
    "assert 'html_content' in df.columns, \"Missing 'html_content' column in data.csv.\"\n",
    "\n",
    "def _remove_boilerplate(soup: BeautifulSoup) -> None:\n",
    "    \"\"\"Remove non-content elements in-place.\"\"\"\n",
    "    for tag in soup(['script','style','noscript','header','footer','nav','aside','form','svg','iframe']):\n",
    "        tag.decompose()\n",
    "\n",
    "def _get_title(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"Prefer <title>, fall back to first <h1>.\"\"\"\n",
    "    if soup.title and soup.title.string:\n",
    "        t = soup.title.string.strip()\n",
    "        if t:\n",
    "            return t\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1:\n",
    "        t = h1.get_text(\" \", strip=True)\n",
    "        if t:\n",
    "            return t\n",
    "    return \"\"\n",
    "\n",
    "def _extract_main_text(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"Prefer article/main/role=main; fall back to semantic tags.\"\"\"\n",
    "    _remove_boilerplate(soup)\n",
    "\n",
    "    # 1) Prefer main content containers\n",
    "    main_candidates = soup.select(\"article, main, [role=main]\")\n",
    "    main_candidates = [c for c in main_candidates if c and c.get_text(strip=True)]\n",
    "    if main_candidates:\n",
    "        # pick the longest text node\n",
    "        best = max(main_candidates, key=lambda el: len(el.get_text(\" \", strip=True)))\n",
    "        text = best.get_text(\" \", strip=True)\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # 2) Fallback: collect from semantic tags\n",
    "    parts: List[str] = []\n",
    "    for sel in ['h1','h2','h3','p','li']:\n",
    "        for t in soup.select(sel):\n",
    "            txt = t.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                parts.append(txt)\n",
    "    text = \" \".join(parts)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def _html_to_title_and_text(html: str) -> Tuple[str, str]:\n",
    "    if not isinstance(html, str) or not html.strip():\n",
    "        return \"\", \"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    title = _get_title(soup)\n",
    "    body_text = _extract_main_text(soup)\n",
    "    return title, body_text\n",
    "\n",
    "def _safe_sent_count(text: str) -> int:\n",
    "    if not text: \n",
    "        return 0\n",
    "    try:\n",
    "        return len(nltk.sent_tokenize(text))\n",
    "    except Exception:\n",
    "        # fallback if punkt isn't available for some reason\n",
    "        return max(1, text.count(\".\") + text.count(\"!\") + text.count(\"?\"))\n",
    "\n",
    "# Process rows\n",
    "records = []\n",
    "for idx, row in df.iterrows():\n",
    "    url = str(row.get(\"url\", \"\"))\n",
    "    html = row.get(\"html_content\", \"\")\n",
    "    try:\n",
    "        title, body = _html_to_title_and_text(html)\n",
    "    except Exception as e:\n",
    "        # Graceful handling per requirements\n",
    "        title, body = \"\", \"\"\n",
    "    wc = len(body.split()) if body else 0\n",
    "    sc = _safe_sent_count(body)\n",
    "    fre = textstat.flesch_reading_ease(body) if body else 0.0\n",
    "    records.append({\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"body_text\": body,\n",
    "        \"word_count\": wc,\n",
    "        \"sentence_count\": sc,\n",
    "        \"flesch_reading_ease\": fre\n",
    "    })\n",
    "\n",
    "# Build DataFrame & save with title included\n",
    "extracted_df = pd.DataFrame.from_records(\n",
    "    records,\n",
    "    columns=[\"url\",\"title\",\"body_text\",\"word_count\",\"sentence_count\",\"flesch_reading_ease\"]\n",
    ")\n",
    "\n",
    "extracted_out = DATA / \"extracted_content.csv\"\n",
    "extracted_df.to_csv(extracted_out, index=False)\n",
    "print(f\"Saved extracted content to: {extracted_out}\")\n",
    "\n",
    "# Quick peek\n",
    "extracted_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a439b",
   "metadata": {},
   "source": [
    "## C. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00a6046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features to: C:\\Users\\AISHWARYA\\Downloads\\seo-content-detector\\data\\features.csv\n",
      "Embedding type: tfidf_svd_50d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>top_keywords</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-91.295000</td>\n",
       "      <td>cybersecurity|blog</td>\n",
       "      <td>[0.055314, 0.050039, -0.036121, -0.000758, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>1747</td>\n",
       "      <td>94</td>\n",
       "      <td>40.871699</td>\n",
       "      <td>varonis|data|access|security|app</td>\n",
       "      <td>[0.258805, 0.390029, -0.082082, 0.026401, -0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>1058</td>\n",
       "      <td>73</td>\n",
       "      <td>53.262918</td>\n",
       "      <td>password|passphrase|cyber defense|authenticati...</td>\n",
       "      <td>[0.205783, 0.302723, -0.096544, 0.04456, -0.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>779</td>\n",
       "      <td>27</td>\n",
       "      <td>1.035698</td>\n",
       "      <td>cisa|cybersecurity|cyber|cybersecurity best|pr...</td>\n",
       "      <td>[0.114343, 0.157992, -0.048894, 0.006224, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  word_count  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog           2   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips        1747   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...        1058   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...         779   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...           0   \n",
       "\n",
       "   sentence_count  flesch_reading_ease  \\\n",
       "0               1           -91.295000   \n",
       "1              94            40.871699   \n",
       "2              73            53.262918   \n",
       "3              27             1.035698   \n",
       "4               0             0.000000   \n",
       "\n",
       "                                        top_keywords  \\\n",
       "0                                 cybersecurity|blog   \n",
       "1                   varonis|data|access|security|app   \n",
       "2  password|passphrase|cyber defense|authenticati...   \n",
       "3  cisa|cybersecurity|cyber|cybersecurity best|pr...   \n",
       "4                                                      \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.055314, 0.050039, -0.036121, -0.000758, -0....  \n",
       "1  [0.258805, 0.390029, -0.082082, 0.026401, -0.2...  \n",
       "2  [0.205783, 0.302723, -0.096544, 0.04456, -0.23...  \n",
       "3  [0.114343, 0.157992, -0.048894, 0.006224, -0.1...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, json, numpy as np, pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- 1) Clean the extracted text (lowercase, collapse whitespace) ---\n",
    "def clean_text(s: str) -> str:\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "texts_raw = extracted_df['body_text'].fillna(\"\")\n",
    "texts_clean = texts_raw.map(clean_text)\n",
    "\n",
    "# If you want to preserve counts computed earlier, keep them.\n",
    "# (They were computed from the unlowercased text; that's fine for the assignment.)\n",
    "# Otherwise, you could recompute counts from texts_clean.\n",
    "\n",
    "# --- 2) TF-IDF on cleaned text (will also power duplicates in Section D) ---\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1,2),\n",
    "    stop_words='english'\n",
    ")\n",
    "X = tfidf.fit_transform(texts_clean)\n",
    "\n",
    "# Backward-compatible feature names\n",
    "if hasattr(tfidf, \"get_feature_names_out\"):\n",
    "    feature_names = np.array(tfidf.get_feature_names_out())\n",
    "else:\n",
    "    feature_names = np.array(tfidf.get_feature_names())\n",
    "\n",
    "# --- 3) Top-5 keywords per document from TF-IDF ---\n",
    "def top_keywords_from_vector(row_matrix, topk=5):\n",
    "    row = row_matrix.tocoo()\n",
    "    if row.nnz == 0:\n",
    "        return []\n",
    "    scores = row.data\n",
    "    idxs = row.col\n",
    "    order = np.argsort(scores)[-topk:][::-1]\n",
    "    return feature_names[idxs[order]].tolist()\n",
    "\n",
    "top_kw = [top_keywords_from_vector(X[i]) for i in range(X.shape[0])]\n",
    "\n",
    "# --- 4) Embeddings column ---\n",
    "# Try Sentence-Transformers (dense semantic embeddings).\n",
    "# Fallback: TruncatedSVD on TF-IDF (50-D) for compact numeric vectors.\n",
    "use_sbert = False\n",
    "emb_matrix = None\n",
    "emb_note = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    # Small, fast, good-quality encoder\n",
    "    sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    emb_matrix = sbert_model.encode(\n",
    "        texts_clean.tolist(),\n",
    "        batch_size=64,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=False,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    use_sbert = True\n",
    "    emb_note = \"sbert_all-MiniLM-L6-v2\"\n",
    "except Exception:\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    # Safe n_components given corpus size/features\n",
    "    n_components = min(50, max(2, min(X.shape[1]-1, X.shape[0]-1)))\n",
    "    if n_components < 2:\n",
    "        # Degenerate case: extremely tiny corpus; make at least 2 dims\n",
    "        n_components = 2\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    emb_matrix = svd.fit_transform(X)\n",
    "    emb_note = f\"tfidf_svd_{n_components}d\"\n",
    "\n",
    "# Serialize each embedding row as a compact JSON list string\n",
    "def to_json_vector(vec: np.ndarray) -> str:\n",
    "    # round to keep file size reasonable\n",
    "    return json.dumps([float(f) for f in np.round(vec, 6)])\n",
    "\n",
    "emb_strings = [to_json_vector(emb_matrix[i]) for i in range(emb_matrix.shape[0])]\n",
    "\n",
    "# --- 5) Assemble features DataFrame ---\n",
    "features_df = extracted_df.copy()\n",
    "# add cleaned text if you want to inspect later (optional)\n",
    "# features_df['body_text_clean'] = texts_clean\n",
    "\n",
    "features_df['top_keywords'] = [ \"|\".join(kws) for kws in top_kw ]  # join to match your example\n",
    "features_df['embedding'] = emb_strings\n",
    "\n",
    "# (Word/sentence/readability already exist in extracted_df.)\n",
    "# Persist exactly the required columns:\n",
    "out_cols = [\n",
    "    'url',\n",
    "    'word_count',\n",
    "    'sentence_count',\n",
    "    'flesch_reading_ease',\n",
    "    'top_keywords',\n",
    "    'embedding'\n",
    "]\n",
    "\n",
    "features_out = DATA / \"features.csv\"\n",
    "features_df[out_cols].to_csv(features_out, index=False)\n",
    "\n",
    "print(f\"Saved features to: {features_out}\")\n",
    "print(\"Embedding type:\", emb_note)\n",
    "features_df[out_cols].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce96e0",
   "metadata": {},
   "source": [
    "## D. Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aaf25fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url1</th>\n",
       "      <th>url2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/SD-WAN</td>\n",
       "      <td>https://www.fortinet.com/resources/cyberglossa...</td>\n",
       "      <td>0.876297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.cisco.com/site/us/en/learn/topics/...</td>\n",
       "      <td>https://www.fortinet.com/resources/cyberglossa...</td>\n",
       "      <td>0.872764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url1  \\\n",
       "0               https://en.wikipedia.org/wiki/SD-WAN   \n",
       "1  https://www.cisco.com/site/us/en/learn/topics/...   \n",
       "\n",
       "                                                url2  similarity  \n",
       "0  https://www.fortinet.com/resources/cyberglossa...    0.876297  \n",
       "1  https://www.fortinet.com/resources/cyberglossa...    0.872764  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cosine similarity on TF-IDF; flag pairs above threshold\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "S = cosine_similarity(X)\n",
    "thresh = 0.85  # adjustable threshold\n",
    "\n",
    "pairs = []\n",
    "n = S.shape[0]\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        sim = float(S[i, j])\n",
    "        if sim >= thresh:\n",
    "            pairs.append((features_df.iloc[i]['url'], features_df.iloc[j]['url'], sim))\n",
    "\n",
    "dup_df = pd.DataFrame(pairs, columns=['url1','url2','similarity'])\n",
    "dup_out = DATA / \"duplicates.csv\"\n",
    "dup_df.to_csv(dup_out, index=False)\n",
    "dup_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3a4d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embeddings for similarity (shape: (81, 50)).\n",
      "Saved duplicate pairs to: C:\\Users\\AISHWARYA\\Downloads\\seo-content-detector\\data\\duplicates.csv\n",
      "Updated features (with is_thin) saved to: C:\\Users\\AISHWARYA\\Downloads\\seo-content-detector\\data\\features.csv\n",
      "\n",
      "Summary:\n",
      "Total pages analyzed: 81\n",
      "Duplicate pairs: 26\n",
      "Thin content pages: 28 (35%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url1</th>\n",
       "      <th>url2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.microsoft.com/en-us/security/busin...</td>\n",
       "      <td>https://www.zscaler.com/resources/security-ter...</td>\n",
       "      <td>0.9866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://copyblogger.com/content-marketing/</td>\n",
       "      <td>https://mailchimp.com/marketing-glossary/conte...</td>\n",
       "      <td>0.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisco.com/site/us/en/learn/topics/...</td>\n",
       "      <td>https://www.fortinet.com/resources/cyberglossa...</td>\n",
       "      <td>0.9825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://en.wikipedia.org/wiki/SD-WAN</td>\n",
       "      <td>https://www.fortinet.com/resources/cyberglossa...</td>\n",
       "      <td>0.9803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://guardiandigital.com/resources/blog/gui...</td>\n",
       "      <td>https://inspiredelearning.com/blog/phishing-pr...</td>\n",
       "      <td>0.9741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://sign.dropbox.com/products/dropbox-fax</td>\n",
       "      <td>https://www.fax.plus/</td>\n",
       "      <td>0.9708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://nytlicensing.com/latest/trends/content...</td>\n",
       "      <td>https://www.twilio.com/en-us/blog/insights/con...</td>\n",
       "      <td>0.9592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://en.wikipedia.org/wiki/SD-WAN</td>\n",
       "      <td>https://www.cisco.com/site/us/en/learn/topics/...</td>\n",
       "      <td>0.9578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://emotive.io/blog/11-essential-digital-m...</td>\n",
       "      <td>https://blog.hubspot.com/marketing/what-is-dig...</td>\n",
       "      <td>0.9529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.forbes.com/advisor/business/what-i...</td>\n",
       "      <td>https://blog.hubspot.com/marketing/what-is-dig...</td>\n",
       "      <td>0.9470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url1  \\\n",
       "0  https://www.microsoft.com/en-us/security/busin...   \n",
       "1         https://copyblogger.com/content-marketing/   \n",
       "2  https://www.cisco.com/site/us/en/learn/topics/...   \n",
       "3               https://en.wikipedia.org/wiki/SD-WAN   \n",
       "4  https://guardiandigital.com/resources/blog/gui...   \n",
       "5      https://sign.dropbox.com/products/dropbox-fax   \n",
       "6  https://nytlicensing.com/latest/trends/content...   \n",
       "7               https://en.wikipedia.org/wiki/SD-WAN   \n",
       "8  https://emotive.io/blog/11-essential-digital-m...   \n",
       "9  https://www.forbes.com/advisor/business/what-i...   \n",
       "\n",
       "                                                url2  similarity  \n",
       "0  https://www.zscaler.com/resources/security-ter...      0.9866  \n",
       "1  https://mailchimp.com/marketing-glossary/conte...      0.9836  \n",
       "2  https://www.fortinet.com/resources/cyberglossa...      0.9825  \n",
       "3  https://www.fortinet.com/resources/cyberglossa...      0.9803  \n",
       "4  https://inspiredelearning.com/blog/phishing-pr...      0.9741  \n",
       "5                              https://www.fax.plus/      0.9708  \n",
       "6  https://www.twilio.com/en-us/blog/insights/con...      0.9592  \n",
       "7  https://www.cisco.com/site/us/en/learn/topics/...      0.9578  \n",
       "8  https://blog.hubspot.com/marketing/what-is-dig...      0.9529  \n",
       "9  https://blog.hubspot.com/marketing/what-is-dig...      0.9470  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, itertools, numpy as np, pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1) Build the representation matrix (embeddings preferred)\n",
    "use_embeddings = False\n",
    "emb_matrix = None\n",
    "\n",
    "if 'embedding' in features_df.columns:\n",
    "    try:\n",
    "        emb_matrix = np.vstack(features_df['embedding'].map(lambda s: np.array(json.loads(s), dtype=np.float32)).values)\n",
    "        if emb_matrix.ndim == 2 and emb_matrix.shape[0] == len(features_df):\n",
    "            use_embeddings = True\n",
    "            print(f\"Using embeddings for similarity (shape: {emb_matrix.shape}).\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse embeddings from features_df['embedding']:\", e)\n",
    "\n",
    "# Fallback to TF-IDF if embeddings are missing/unusable\n",
    "if not use_embeddings:\n",
    "    if 'X' in globals() and X is not None:\n",
    "        from sklearn.preprocessing import normalize\n",
    "        emb_matrix = X  # sparse CSR\n",
    "        print(f\"Falling back to TF-IDF matrix for similarity (shape: {emb_matrix.shape}).\")\n",
    "    else:\n",
    "        raise RuntimeError(\"No embeddings/TF-IDF matrix available. Run Section C first.\")\n",
    "\n",
    "# 2) Cosine similarity matrix\n",
    "# For sparse TF-IDF, cosine_similarity handles sparse input directly.\n",
    "S = cosine_similarity(emb_matrix)\n",
    "\n",
    "# 3) Threshold and pair extraction\n",
    "thresh = 0.80  # as per requirement/example\n",
    "pairs = []\n",
    "n = S.shape[0]\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        sim = float(S[i, j])\n",
    "        if sim >= thresh:\n",
    "            pairs.append((features_df.iloc[i]['url'], features_df.iloc[j]['url'], round(sim, 4)))\n",
    "\n",
    "# Sort pairs by similarity descending for readability\n",
    "pairs.sort(key=lambda t: t[2], reverse=True)\n",
    "\n",
    "# 4) Save duplicates.csv with EXACT columns: url1,url2,similarity\n",
    "dup_df = pd.DataFrame(pairs, columns=['url1','url2','similarity'])\n",
    "dup_out = DATA / \"duplicates.csv\"\n",
    "dup_df.to_csv(dup_out, index=False)\n",
    "print(f\"Saved duplicate pairs to: {dup_out}\")\n",
    "\n",
    "# 5) Thin content detection -> is_thin column and update features.csv\n",
    "features_df['is_thin'] = (features_df['word_count'] < 500).astype(int)\n",
    "\n",
    "# Persist updated features with is_thin included (add column if missing in file)\n",
    "features_out = DATA / \"features.csv\"\n",
    "cols = ['url','word_count','sentence_count','flesch_reading_ease','top_keywords','embedding','is_thin']\n",
    "existing_cols = [c for c in cols if c in features_df.columns]\n",
    "features_df[existing_cols].to_csv(features_out, index=False)\n",
    "print(f\"Updated features (with is_thin) saved to: {features_out}\")\n",
    "\n",
    "# 6) Print summary (like the example)\n",
    "total_pages = len(features_df)\n",
    "duplicate_pairs = len(dup_df)\n",
    "thin_pages = int(features_df['is_thin'].sum())\n",
    "thin_pct = (thin_pages / total_pages * 100.0) if total_pages else 0.0\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total pages analyzed: {total_pages}\")\n",
    "print(f\"Duplicate pairs: {duplicate_pairs}\")\n",
    "print(f\"Thin content pages: {thin_pages} ({thin_pct:.0f}%)\")\n",
    "\n",
    "# Peek at the top of duplicates\n",
    "dup_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635a503",
   "metadata": {},
   "source": [
    "## E. Quality Labels & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5862675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (rule-based on word_count only) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.154     1.000     0.267         2\n",
      "         Low      1.000     0.429     0.600        14\n",
      "      Medium      0.167     0.111     0.133         9\n",
      "\n",
      "    accuracy                          0.360        25\n",
      "   macro avg      0.440     0.513     0.333        25\n",
      "weighted avg      0.632     0.360     0.405        25\n",
      "\n",
      "Confusion matrix:\n",
      " [[2 0 0]\n",
      " [3 6 5]\n",
      " [8 0 1]]\n",
      "Baseline Accuracy: 0.360 | Baseline Macro F1: 0.333\n",
      "\n",
      "=== LogisticRegression ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.286     1.000     0.444         2\n",
      "         Low      1.000     0.929     0.963        14\n",
      "      Medium      1.000     0.556     0.714         9\n",
      "\n",
      "    accuracy                          0.800        25\n",
      "   macro avg      0.762     0.828     0.707        25\n",
      "weighted avg      0.943     0.800     0.832        25\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 2  0  0]\n",
      " [ 1 13  0]\n",
      " [ 4  0  5]]\n",
      "Accuracy: 0.800 | Macro F1: 0.707\n",
      "\n",
      "=== RandomForest ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.667     1.000     0.800         2\n",
      "         Low      1.000     0.929     0.963        14\n",
      "      Medium      0.889     0.889     0.889         9\n",
      "\n",
      "    accuracy                          0.920        25\n",
      "   macro avg      0.852     0.939     0.884        25\n",
      "weighted avg      0.933     0.920     0.923        25\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 2  0  0]\n",
      " [ 0 13  1]\n",
      " [ 1  0  8]]\n",
      "Accuracy: 0.920 | Macro F1: 0.884\n",
      "\n",
      "=== Best Model Summary ===\n",
      "Best Model: RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High      0.667     1.000     0.800         2\n",
      "         Low      1.000     0.929     0.963        14\n",
      "      Medium      0.889     0.889     0.889         9\n",
      "\n",
      "    accuracy                          0.920        25\n",
      "   macro avg      0.852     0.939     0.884        25\n",
      "weighted avg      0.933     0.920     0.923        25\n",
      "\n",
      "Best model confusion matrix:\n",
      " [[ 2  0  0]\n",
      " [ 0 13  1]\n",
      " [ 1  0  8]]\n",
      "Overall Accuracy: 0.920 | Macro F1: 0.884\n",
      "Top Features:\n",
      "1. flesch_reading_ease (importance: 0.446)\n",
      "2. word_count (importance: 0.278)\n",
      "3. sentence_count (importance: 0.275)\n",
      "\n",
      "Saved best model to: C:\\Users\\AISHWARYA\\Downloads\\seo-content-detector\\models\\quality_model.pkl\n",
      "Compare vs Baseline -> Accuracy Δ: +0.560, Macro F1 Δ: +0.551\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# ---- 1) Create synthetic labels (non-overlapping)\n",
    "def make_quality_label(wc, fre):\n",
    "    if (wc > 1500) and (50 <= fre <= 70):\n",
    "        return \"High\"\n",
    "    if (wc < 500) or (fre < 30):\n",
    "        return \"Low\"\n",
    "    return \"Medium\"\n",
    "\n",
    "# features_df must exist from Section C; extracted_df from Section B\n",
    "# (features_df already has word_count, sentence_count, flesch_reading_ease)\n",
    "labeled_df = features_df.copy()\n",
    "labeled_df['quality_label'] = [\n",
    "    make_quality_label(wc, fre) \n",
    "    for wc, fre in zip(labeled_df['word_count'], labeled_df['flesch_reading_ease'])\n",
    "]\n",
    "\n",
    "# ---- 2) Define features (you can add more; keep the core 3)\n",
    "feature_cols = ['word_count','sentence_count','flesch_reading_ease']\n",
    "X = labeled_df[feature_cols].fillna(0).astype(float).values\n",
    "y = labeled_df['quality_label'].values\n",
    "\n",
    "# Ensure we have enough data\n",
    "if len(labeled_df) < 10 or labeled_df['quality_label'].nunique() < 2:\n",
    "    raise RuntimeError(\"Not enough data or label variety to train a classifier.\")\n",
    "\n",
    "# ---- 3) Train/test 70/30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---- 4) Baseline (rule-based) using word_count only\n",
    "def baseline_predict_wordcount_only(wc):\n",
    "    if wc < 500:\n",
    "        return \"Low\"\n",
    "    elif wc > 1500:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Medium\"\n",
    "\n",
    "y_pred_baseline = [\n",
    "    baseline_predict_wordcount_only(wc) for wc in labeled_df.loc[y_test.index if hasattr(y_test,'index') else labeled_df.index[:len(y_test)], 'word_count']\n",
    "] if hasattr(y_test, 'index') else [baseline_predict_wordcount_only(wc) for wc in X_test[:, 0]]\n",
    "\n",
    "# If the above indexing is confusing, recompute baseline on X_test directly (word_count is col 0):\n",
    "y_pred_baseline = [baseline_predict_wordcount_only(wc) for wc in X_test[:, 0]]\n",
    "\n",
    "base_acc = accuracy_score(y_test, y_pred_baseline)\n",
    "base_f1  = f1_score(y_test, y_pred_baseline, average='macro')\n",
    "\n",
    "print(\"=== Baseline (rule-based on word_count only) ===\")\n",
    "print(classification_report(y_test, y_pred_baseline, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_baseline))\n",
    "print(f\"Baseline Accuracy: {base_acc:.3f} | Baseline Macro F1: {base_f1:.3f}\")\n",
    "\n",
    "# ---- 5) Train models\n",
    "# Logistic Regression (class_weight balanced helps rare 'High')\n",
    "logreg = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=2000, multi_class='auto', class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400, max_depth=None, min_samples_leaf=2,\n",
    "    class_weight='balanced', random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "models = [\n",
    "    (\"LogisticRegression\", logreg),\n",
    "    (\"RandomForest\", rf),\n",
    "]\n",
    "\n",
    "best = None\n",
    "best_scores = (-1, -1)  # (accuracy, macro_f1)\n",
    "best_report = \"\"\n",
    "best_cm = None\n",
    "best_name = \"\"\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    mf1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(f\"Accuracy: {acc:.3f} | Macro F1: {mf1:.3f}\")\n",
    "    if (acc, mf1) > best_scores:\n",
    "        best_scores = (acc, mf1)\n",
    "        best = model\n",
    "        best_name = name\n",
    "        best_report = classification_report(y_test, y_pred, digits=3)\n",
    "        best_cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# ---- 6) Top features (2–3)\n",
    "def top_features_for_model(model, feature_names, topk=3):\n",
    "    tops = []\n",
    "    if isinstance(model, Pipeline):\n",
    "        # unwrap\n",
    "        clf = model.named_steps.get('clf', model)\n",
    "    else:\n",
    "        clf = model\n",
    "\n",
    "    if isinstance(clf, LogisticRegression):\n",
    "        # importance = mean absolute coef across classes\n",
    "        coefs = np.abs(clf.coef_)  # shape: [n_classes, n_features]\n",
    "        imp = coefs.mean(axis=0)\n",
    "        order = np.argsort(imp)[::-1][:topk]\n",
    "        tops = [(feature_names[i], float(imp[i])) for i in order]\n",
    "    elif isinstance(clf, RandomForestClassifier):\n",
    "        imp = clf.feature_importances_\n",
    "        order = np.argsort(imp)[::-1][:topk]\n",
    "        tops = [(feature_names[i], float(imp[i])) for i in order]\n",
    "    return tops\n",
    "\n",
    "feature_names = feature_cols\n",
    "tops = top_features_for_model(best, feature_names, topk=3)\n",
    "\n",
    "print(\"\\n=== Best Model Summary ===\")\n",
    "print(f\"Best Model: {best_name}\")\n",
    "print(best_report)\n",
    "print(\"Best model confusion matrix:\\n\", best_cm)\n",
    "print(f\"Overall Accuracy: {best_scores[0]:.3f} | Macro F1: {best_scores[1]:.3f}\")\n",
    "print(\"Top Features:\")\n",
    "for i, (fname, score) in enumerate(tops, 1):\n",
    "    print(f\"{i}. {fname} (importance: {score:.3f})\")\n",
    "\n",
    "# ---- 7) Save best model\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best, MODELS / \"quality_model.pkl\")\n",
    "print(\"\\nSaved best model to:\", MODELS / \"quality_model.pkl\")\n",
    "print(\"Compare vs Baseline -> Accuracy Δ: \"\n",
    "      f\"{(best_scores[0]-base_acc):+.3f}, Macro F1 Δ: {(best_scores[1]-base_f1):+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70035998",
   "metadata": {},
   "source": [
    "## F. Real-time `analyze_url(url)` Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7c074b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, time, requests, numpy as np, pandas as pd\n",
    "from typing import List, Tuple\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- polite fetch with UA + error handling ---\n",
    "def _fetch_html(url: str, timeout: int = 20) -> str:\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (SEO-Assignment/1.0; +https://example.com)\"}\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"\"\n",
    "\n",
    "# --- minimal boilerplate removal + main-content extraction (same approach as Section B) ---\n",
    "def _remove_boilerplate(soup: BeautifulSoup) -> None:\n",
    "    for tag in soup(['script','style','noscript','header','footer','nav','aside','form','svg','iframe']):\n",
    "        tag.decompose()\n",
    "\n",
    "def _get_title(soup: BeautifulSoup) -> str:\n",
    "    if soup.title and soup.title.string:\n",
    "        t = soup.title.string.strip()\n",
    "        if t: return t\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1:\n",
    "        t = h1.get_text(\" \", strip=True)\n",
    "        if t: return t\n",
    "    return \"\"\n",
    "\n",
    "def _extract_main_text(soup: BeautifulSoup) -> str:\n",
    "    _remove_boilerplate(soup)\n",
    "    # main/article/role=main first\n",
    "    cands = soup.select(\"article, main, [role=main]\")\n",
    "    cands = [c for c in cands if c and c.get_text(strip=True)]\n",
    "    if cands:\n",
    "        best = max(cands, key=lambda el: len(el.get_text(\" \", strip=True)))\n",
    "        text = best.get_text(\" \", strip=True)\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # fallback: semantic tags\n",
    "    parts: List[str] = []\n",
    "    for sel in ['h1','h2','h3','p','li']:\n",
    "        for t in soup.select(sel):\n",
    "            txt = t.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                parts.append(txt)\n",
    "    return re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip()\n",
    "\n",
    "def _html_to_text(html: str) -> Tuple[str, str]:\n",
    "    if not isinstance(html, str) or not html.strip():\n",
    "        return \"\", \"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    title = _get_title(soup)\n",
    "    body  = _extract_main_text(soup)\n",
    "    return title, body\n",
    "\n",
    "def _safe_sent_count(text: str) -> int:\n",
    "    if not text: return 0\n",
    "    try:\n",
    "        return len(nltk.sent_tokenize(text))\n",
    "    except Exception:\n",
    "        return max(1, text.count(\".\") + text.count(\"!\") + text.count(\"?\"))\n",
    "\n",
    "# --- quality labeling: try trained model; fallback to rule-based ---\n",
    "def _rule_quality_label(wc: int, fre: float) -> str:\n",
    "    if (wc > 1500) and (50 <= fre <= 70):\n",
    "        return \"High\"\n",
    "    if (wc < 500) or (fre < 30):\n",
    "        return \"Low\"\n",
    "    return \"Medium\"\n",
    "\n",
    "def _predict_quality(word_count: int, sentence_count: int, flesch: float):\n",
    "    x = np.array([[float(word_count), float(sentence_count), float(flesch)]])\n",
    "    # Try using the best trained model from Section E (variable `best`) or load from disk\n",
    "    model = None\n",
    "    if 'best' in globals() and best is not None:\n",
    "        model = best\n",
    "    else:\n",
    "        try:\n",
    "            import joblib\n",
    "            model = joblib.load(MODELS / \"quality_model.pkl\")\n",
    "        except Exception:\n",
    "            model = None\n",
    "    if model is not None:\n",
    "        try:\n",
    "            return model.predict(x)[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return _rule_quality_label(word_count, flesch)\n",
    "\n",
    "# --- vectorization for similarity ---\n",
    "# Priority: embeddings vs. corpus embeddings; fallback to TF-IDF space\n",
    "def _vectorize_for_similarity(text: str):\n",
    "    text_clean = re.sub(r\"\\s+\", \" \", text.lower()).strip()\n",
    "    # 1) SBERT available?\n",
    "    if 'sbert_model' in globals() and sbert_model is not None:\n",
    "        try:\n",
    "            vec = sbert_model.encode([text_clean], convert_to_numpy=True, normalize_embeddings=True)\n",
    "            return vec, 'sbert'\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 2) SVD (TF-IDF -> compact embedding) available?\n",
    "    if 'svd' in globals() and svd is not None:\n",
    "        try:\n",
    "            row = tfidf.transform([text_clean])  # needs Section C to have built 'tfidf'\n",
    "            vec = svd.transform(row)\n",
    "            return vec, 'svd'\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 3) Fallback: raw TF-IDF vector\n",
    "    row = tfidf.transform([text_clean])\n",
    "    return row, 'tfidf'\n",
    "\n",
    "def _corpus_matrix_and_urls():\n",
    "    # if we saved embeddings as strings, parse them\n",
    "    if 'embedding' in features_df.columns:\n",
    "        try:\n",
    "            emb_matrix = np.vstack(features_df['embedding'].map(lambda s: np.array(json.loads(s), dtype=np.float32)).values)\n",
    "            return emb_matrix, features_df['url'].tolist(), 'embeddings'\n",
    "        except Exception:\n",
    "            pass\n",
    "    # else use TF-IDF matrix X (global from Section C)\n",
    "    return X, features_df['url'].tolist(), 'tfidf'\n",
    "\n",
    "def analyze_url(url: str, top_k: int = 3, dup_threshold: float = 0.80):\n",
    "    html = _fetch_html(url)\n",
    "    if not html:\n",
    "        return {\"url\": url, \"error\": \"Failed to fetch the page.\"}\n",
    "\n",
    "    title, body = _html_to_text(html)\n",
    "    wc = len(body.split()) if body else 0\n",
    "    sc = _safe_sent_count(body)\n",
    "    fre = textstat.flesch_reading_ease(body) if body else 0.0\n",
    "\n",
    "    quality = _predict_quality(wc, sc, fre)\n",
    "    is_thin = wc < 500\n",
    "\n",
    "    # Vectorize this page and compare to corpus\n",
    "    q_vec, q_kind = _vectorize_for_similarity(body)\n",
    "    corpus_mat, corpus_urls, corpus_kind = _corpus_matrix_and_urls()\n",
    "\n",
    "    sims = cosine_similarity(q_vec, corpus_mat)[0]\n",
    "    order = np.argsort(sims)[::-1]\n",
    "\n",
    "    similar_list = []\n",
    "    # collect matches >= threshold\n",
    "    for idx in order:\n",
    "        sim = float(sims[idx])\n",
    "        if sim >= dup_threshold:\n",
    "            similar_list.append({\"url\": corpus_urls[idx], \"similarity\": round(sim, 4)})\n",
    "            if len(similar_list) >= top_k:\n",
    "                break\n",
    "\n",
    "    # if none exceed threshold, still provide the single most similar for context\n",
    "    if not similar_list and len(order) > 0:\n",
    "        best_idx = int(order[0])\n",
    "        similar_list = [{\"url\": corpus_urls[best_idx], \"similarity\": round(float(sims[best_idx]), 4)}]\n",
    "\n",
    "    result = {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"word_count\": wc,\n",
    "        \"readability\": round(float(fre), 3),\n",
    "        \"quality_label\": quality,\n",
    "        \"is_thin\": bool(is_thin),\n",
    "        \"similar_to\": similar_list\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0022def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a URL to analyze: https://thehackernews.com/2025/10/openai-unveils-aardvark-gpt-5-agent.html\n",
      "{\n",
      "  \"url\": \"https://thehackernews.com/2025/10/openai-unveils-aardvark-gpt-5-agent.html\",\n",
      "  \"title\": \"OpenAI Unveils Aardvark: GPT-5 Agent That Finds and Fixes Code Flaws Automatically\",\n",
      "  \"word_count\": 721,\n",
      "  \"readability\": 18.367,\n",
      "  \"quality_label\": \"Low\",\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": [\n",
      "    {\n",
      "      \"url\": \"https://www.fortinet.com/resources/cyberglossary/what-is-network-security\",\n",
      "      \"similarity\": 0.6572\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# === Interactive prompt  ===\n",
    "import re, json\n",
    "\n",
    "def _normalize_url(u: str) -> str:\n",
    "    u = (u or \"\").strip()\n",
    "    if not u:\n",
    "        return u\n",
    "    if not re.match(r\"^https?://\", u, flags=re.I):\n",
    "        u = \"https://\" + u\n",
    "    return u\n",
    "\n",
    "user_url = input(\"Enter a URL to analyze: \").strip()\n",
    "user_url = _normalize_url(user_url)\n",
    "\n",
    "if not user_url:\n",
    "    print(\"No URL provided.\")\n",
    "else:\n",
    "    result = analyze_url(user_url)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3bf7ec",
   "metadata": {},
   "source": [
    "## G. Save Outputs (CSV + model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc699c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - C:\\Users\\AISHWARYA\\Downloads\\seo-content-detector\\data\\extracted_content.csv\n",
      " - C:\\Users\\AISHWARYA\\Downloads\\seo-content-detector\\data\\features.csv\n",
      " - C:\\Users\\AISHWARYA\\Downloads\\seo-content-detector\\data\\duplicates.csv\n",
      "Model (if trained): C:\\Users\\AISHWARYA\\Downloads\\seo-content-detector\\models\\quality_model.pkl True\n"
     ]
    }
   ],
   "source": [
    "# Ensure outputs are saved (already written earlier)\n",
    "extracted_df.to_csv(DATA / \"extracted_content.csv\", index=False)\n",
    "features_df[['url','word_count','sentence_count','flesch_reading_ease','top_keywords']].to_csv(DATA / \"features.csv\", index=False)\n",
    "dup_df.to_csv(DATA / \"duplicates.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", DATA / \"extracted_content.csv\")\n",
    "print(\" -\", DATA / \"features.csv\")\n",
    "print(\" -\", DATA / \"duplicates.csv\")\n",
    "print(\"Model (if trained):\", MODELS / \"quality_model.pkl\", (MODELS / \"quality_model.pkl\").exists())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
